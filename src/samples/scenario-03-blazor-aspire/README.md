# Scenario 04 â€” Blazor + Aspire + Ollama Conversation

A real-time conversation app that combines a **Blazor Server** frontend with an **Ollama-powered** AI backend, orchestrated by **.NET Aspire**.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  .NET Aspire AppHost                     â”‚
â”‚         (Orchestration Â· Dashboard Â· Telemetry)          â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                  â”‚                  â”‚
   â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Blazor Web  â”‚  â”‚   API Backend â”‚  â”‚    Ollama     â”‚
â”‚  (Server)    â”‚  â”‚  (ASP.NET     â”‚  â”‚  (Container)  â”‚
â”‚              â”‚  â”‚   Core)       â”‚  â”‚  phi4-mini    â”‚
â”‚  Chat UI     â”‚  â”‚  SignalR Hub  â”‚  â”‚              â”‚
â”‚  SignalR     â”‚  â”‚  M.E.AI       â”‚  â”‚  REST API    â”‚
â”‚  Client      â”‚  â”‚  Streaming    â”‚  â”‚  :11434      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                  â”‚
       â”‚  SignalR        â”‚  OpenAI-compat   â”‚
       â”‚  (streaming)    â”‚  HTTP API        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”˜
```

### Data Flow

```
User types message
    â”‚
    â–¼
Blazor (SignalR) â”€â”€â–º API Hub â”€â”€â–º M.E.AI â”€â”€â–º Ollama (phi4-mini)
                                                â”‚
                                    streaming tokens
                                                â”‚
User sees response â—„â”€â”€ Blazor â—„â”€â”€ SignalR â—„â”€â”€â”€â”€â”€â”˜
```

## Prerequisites

1. **.NET 10 SDK** (or .NET 9 SDK)
2. **Ollama** installed and running locally â€” [ollama.com](https://ollama.com)
3. **phi4-mini model** pulled: `ollama pull phi4-mini`

## How to Run

```bash
# 1. Start Ollama (if not already running):
ollama serve

# 2. Pull the model (first time only):
ollama pull phi4-mini

# 3. From the repo root:
cd src/samples/scenario-04-blazor-aspire

# 4. Run the Aspire AppHost (starts API + Web):
dotnet run --project scenario-04.AppHost
```

### What happens when you run it:

1. **Aspire starts the API backend** â€” connects to Ollama at `http://localhost:11434`, exposes SignalR hub
2. **Aspire starts the Blazor frontend** â€” connects to API via SignalR
3. **Aspire Dashboard opens** â€” shows all services, logs, traces

### Using Docker-managed Ollama (optional)

If you prefer Aspire to manage Ollama via Docker instead of running it locally, edit `scenario-04.AppHost/Program.cs` and uncomment the Docker-based Ollama section. This requires Docker Desktop to be running.

## Using the App

1. Open the **Blazor Web** endpoint from the Aspire dashboard (or the URL printed in console)
2. Navigate to `/conversation`
3. Type a message and press Enter or click Send
4. Watch the AI response stream in real-time, token by token

### Features

- **Streaming responses** â€” tokens appear as Ollama generates them
- **Multi-turn conversation** â€” context is maintained across messages
- **Custom persona** â€” set a system prompt (e.g., "You are a pirate captain")
- **Session management** â€” clear history and start fresh
- **Connection status** â€” visual indicator for SignalR connection health
- **ğŸ—£ï¸ Speak Mode** â€” always-on microphone with automatic turn detection (GPT-Realtime-like hands-free conversation)
- **ğŸ¤ Push-to-talk** â€” single utterance voice input
- **ğŸ”Š Auto-speak** â€” AI responses spoken aloud via browser TTS

### Voice Modes

| Mode | How it works | Best for |
|------|-------------|----------|
| **Text** | Type and press Enter/Send | Normal chat |
| **Push-to-talk (ğŸ¤)** | Click mic â†’ speak â†’ auto-sends on pause | Quick voice input |
| **Speak Mode (ğŸ—£ï¸)** | Click to enter always-on mode. Mic stays open, auto-sends on each pause, AI speaks response, mic resumes listening. Click ğŸ”´ or â¹ï¸ Stop to exit. | Hands-free conversation |

In Speak Mode, the state indicator shows:
- ğŸŸ¢ **Listening** â€” mic is open, waiting for you to speak
- ğŸ¤ **Hearing you...** â€” speech detected, transcribing
- â³ **Processing** â€” sending to Ollama
- ğŸ”Š **Speaking** â€” AI is responding (interrupt by speaking again)

## Key Technology Choices

| Component | Technology | Version | Why |
|-----------|-----------|---------|-----|
| Frontend | **Blazor Server** | .NET 10 | SignalR built-in, server-side rendering |
| Communication | **SignalR + MessagePack** | 10.0.3 | Binary streaming, auto-reconnect |
| AI Framework | **Microsoft Agent Framework** | 1.0.0-rc2 | `AIAgent` + `OllamaChatClient` ([docs](https://learn.microsoft.com/agent-framework/agents/providers/ollama)) |
| AI Abstractions | **Microsoft.Extensions.AI.Ollama** | 9.7.0-preview | `OllamaChatClient` as `IChatClient` |
| LLM | **Ollama (phi4-mini)** | latest | 3.8B params, fast, runs locally |
| Orchestration | **.NET Aspire** | 13.1.2 | Service discovery, telemetry, container management |

## Microsoft Agent Framework Integration

This scenario follows the [official Microsoft Agent Framework + Ollama pattern](https://learn.microsoft.com/agent-framework/agents/providers/ollama).

### How it works

**1. Register OllamaChatClient as IChatClient (Program.cs):**

```csharp
// Microsoft.Extensions.AI.Ollama provides OllamaChatClient
builder.Services.AddChatClient(new OllamaChatClient(
        new Uri(ollamaEndpoint), ollamaModel))
    .UseFunctionInvocation()    // Enable function/tool calling
    .UseOpenTelemetry()         // Traces visible in Aspire dashboard
    .UseLogging();              // Log all AI interactions
```

**2. One-shot agent query (Agent Framework pattern):**

```csharp
using Microsoft.Agents.AI;

// Create an AIAgent from the IChatClient â€” this is the Agent Framework pattern
var agent = chatClient.AsAIAgent(
    instructions: "You are a helpful assistant running locally via Ollama.");

var result = await agent.RunAsync("What is the largest city in France?");
Console.WriteLine(result.Text);
```

**3. Multi-turn streaming conversation (ConversationService):**

```csharp
// For multi-turn chat, we manage history per session and stream tokens
await foreach (var token in chatClient.GetStreamingResponseAsync(chatHistory))
{
    yield return token.Text;  // Stream each token to the Blazor UI via SignalR
}
```

### Packages used

```xml
<PackageReference Include="Microsoft.Extensions.AI" Version="10.3.0" />
<PackageReference Include="Microsoft.Extensions.AI.Ollama" Version="9.7.0-preview.1.25356.2" />
<PackageReference Include="Microsoft.Agents.AI" Version="1.0.0-rc2" />
```

## Changing the Ollama Model

Pull a different model and update `scenario-04.Api/appsettings.json` (or set the `Ollama:Model` config):

```bash
ollama pull llama3.2
```

Then set the model name in the API config or environment variable:

```json
{
  "Ollama": {
    "Endpoint": "http://localhost:11434",
    "Model": "llama3.2"
  }
}
```

Popular options:
| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `phi4-mini` | ~2.5 GB | âš¡ Fast | Good |
| `llama3.2` | ~2 GB | âš¡ Fast | Good |
| `llama3.1:8b` | ~4.7 GB | Medium | Better |
| `phi4` | ~9 GB | Slower | Best |

## Project Structure

```
scenario-04-blazor-aspire/
â”œâ”€â”€ scenario-04.AppHost/           # Aspire orchestrator
â”‚   â””â”€â”€ Program.cs                 # Ollama + API + Web wiring
â”œâ”€â”€ scenario-04.ServiceDefaults/   # Shared telemetry/health
â”‚   â””â”€â”€ Extensions.cs
â”œâ”€â”€ scenario-04.Api/               # ASP.NET Core backend
â”‚   â”œâ”€â”€ Program.cs                 # DI, SignalR, M.E.AI setup
â”‚   â”œâ”€â”€ Hubs/
â”‚   â”‚   â””â”€â”€ ConversationHub.cs     # SignalR hub (streaming)
â”‚   â””â”€â”€ Services/
â”‚       â””â”€â”€ ConversationService.cs # Multi-turn chat with Ollama
â”œâ”€â”€ scenario-04.Web/               # Blazor Server frontend
â”‚   â”œâ”€â”€ Program.cs
â”‚   â”œâ”€â”€ Components/
â”‚   â”‚   â”œâ”€â”€ App.razor
â”‚   â”‚   â”œâ”€â”€ Routes.razor
â”‚   â”‚   â”œâ”€â”€ Layout/MainLayout.razor
â”‚   â”‚   â””â”€â”€ Pages/
â”‚   â”‚       â”œâ”€â”€ Index.razor        # Home page
â”‚   â”‚       â””â”€â”€ Conversation.razor # Chat UI
â”‚   â””â”€â”€ wwwroot/css/app.css
â””â”€â”€ scenario-04.Shared/            # Shared DTOs
    â””â”€â”€ Models/
        â”œâ”€â”€ AudioChunkDto.cs
        â”œâ”€â”€ ChatMessageDto.cs
        â””â”€â”€ ConversationStateDto.cs
```

## Future: PersonaPlex Audio Integration

When the PersonaPlex ONNX models are fully exported, this scenario will be extended to support:

```
User speaks â†’ Mimi Encoder â†’ Ollama reasoning â†’ Mimi Decoder â†’ AI speaks back
```

The `ConversationHub.ProcessAudio()` method has a placeholder ready for this integration. See the [evaluation document](../../../docs/scenario-04-blazor-aspire-evaluation.md) for the full roadmap.
